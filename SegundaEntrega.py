# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rBZ4tOhPFLgmoHMzyvnH5NkuMIop6Ked
"""

#Cargamos los datos del dataset

import pandas as pd

data = pd.read_csv('Cars93.csv')
data.keys()

# Observamos la cabecera del dataset

data.head(5)

# Verificamos si existen celdas vacias
data.isnull().sum()

#Seleccionamos algunas features y el objetivo

keys = ['Manufacturer', 'Model', 'Type', 'Min.Price',
       'Max.Price', 'MPG.city', 'MPG.highway', 'AirBags', 'DriveTrain',
       'Cylinders', 'EngineSize', 'Horsepower', 'RPM', 'Rev.per.mile',
       'Man.trans.avail', 'Fuel.tank.capacity', 'Passengers', 'Length',
       'Wheelbase', 'Width', 'Turn.circle', 'Rear.seat.room', 'Luggage.room',
       'Weight', 'Origin', 'Make']
y = 'Price'

#Visualizando algunos datos
fig = plt.figure(figsize=(7,5))
plt.hist(data['Cylinders'])
plt.xlabel('Cylinders')
plt.ylabel('Price')
plt.show()

#Visualizando el objetivo
fig = plt.figure(figsize=(7,5))
plt.hist(data['Price'])
plt.xlabel('Price')
plt.ylabel('counts')
plt.show()

#Clasificación Visual
for k in keys:
  fig = plt.figure(figsize=(7,5))
  plt.plot(data[k],data[y],'*')
  plt.xlabel(k)
  plt.ylabel(y)
  plt.show()

Y = np.array(data[y], dtype=int)
np.unique(Y,return_counts=True)

Y[Y<(np.mean(data['Price']))] = 0
Y[Y>=(np.mean(data['Price']))] = 1

np.unique(Y,return_counts=True)

for k in keys:
  fig = plt.figure(figsize=(7,5))
  plt.plot(data[k][Y==0],Y[Y==0],'*')
  plt.plot(data[k][Y==1],Y[Y==1],'.')
  plt.xlabel(k)
  plt.ylabel(y)
  plt.show()

!pip install corner

keys_categoric=['Manufacturer', 'Model', 'Type', 'AirBags', 'DriveTrain',
       'Man.trans.avail','Origin', 'Make']
keys_categoric_codes=['Manufacturer_code', 'Model_code', 'Type_code', 'AirBags_code', 'DriveTrain_code',
       'Man.trans.avail_code','Origin_code', 'Make_code']

features=[]

for k in keys_categoric:
  categoric_clases=data[k].value_counts().index.tolist()
  features.append(categoric_clases)

#Codificamos los features categoricos

from sklearn.preprocessing import OrdinalEncoder

for i in range(len(keys_categoric)):

  # Creamos el codificador indicandole el orden de la variables
  encoder = OrdinalEncoder(categories=[features[i]])

  # Ajustamos el codificador con la variable education y la transformamos
  encoder.fit(data[[keys_categoric[i]]])
  data[keys_categoric_codes[i]] = encoder.transform(data[[keys_categoric[i]]])

keys_code_use = ['Manufacturer_code', 'Model_code', 'Min.Price',
       'Max.Price', 'MPG.city', 'MPG.highway','Make_code' ]

import corner

figure = corner.corner(data[keys_code_use], labels=keys_code_use,
                       quantiles=[0.16, 0.5, 0.84], show_titles=True, 
                       title_kwargs={"fontsize": 12})

from sklearn.preprocessing import StandardScaler

data_scaled = StandardScaler().fit_transform(data[keys_code_use])

import corner

figure = corner.corner(data_scaled, labels=keys_code_use,
                       quantiles=[0.16, 0.5, 0.84],
                       show_titles=True, title_kwargs={"fontsize": 12})

#--- Cargamos el método para crear un modelo de Bosque Aleatorio

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model

#--- Importamos algunos métodos de sklearn

from sklearn.model_selection import train_test_split

model = RandomForestClassifier(n_estimators=100, max_depth=10)

X_train, X_test, Y_train, Y_test = train_test_split(np.array(data[keys_code_use]),y)

model.fit(X_train,Y_train)

Y_pred = model.predict(X_test)

#--- Calculamos la precisión
acc = sum(Y_pred == Y_test)/len(Y_test)
acc

#--- Calculamos la importancia de los parametros
importances = model.feature_importances_
importances

#--- Organizamos los parametros de menor a mayor
ii = np.argsort(importances)[::-1]
np.array(keys_code_use)[ii]

